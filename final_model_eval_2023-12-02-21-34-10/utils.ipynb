{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '{\"is_fraudulent\": false}', 'predict': '{\"is_fraudulent\": false}'},\n",
       " {'label': '{\"is_fraudulent\": false}', 'predict': '{\"is_fraudulent\": false}'},\n",
       " {'label': '{\"is_fraudulent\": false}', 'predict': '{\"is_fraudulent\": false}'},\n",
       " {'label': '{\"is_fraudulent\": true}', 'predict': '{\"is_fraudulent\": true}'},\n",
       " {'label': '{\"is_fraudulent\": false}', 'predict': '{\"is_fraudulent\": false}'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Re-load the generated predictions file due to code execution reset\n",
    "predictions_file_path = 'generated_predictions.jsonl'\n",
    "with open(predictions_file_path, 'r') as file:\n",
    "    predictions = [json.loads(line) for line in file]\n",
    "\n",
    "# Display the first few entries to understand the structure\n",
    "predictions[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: 0.9937219730941704, 0.9863013698630136, 0.9664429530201343, 0.9762711864406781\n",
      "\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.97\n",
      "F1 Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Extracting labels and predictions\n",
    "labels = [json.loads(entry['label'])['is_fraudulent'] for entry in predictions]\n",
    "predicts = [json.loads(entry['predict'])['is_fraudulent'] for entry in predictions]\n",
    "\n",
    "# Converting boolean to integer for metrics calculation\n",
    "labels = [int(label) for label in labels]\n",
    "predicts = [int(predict) for predict in predicts]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(labels, predicts)\n",
    "precision = precision_score(labels, predicts)\n",
    "recall = recall_score(labels, predicts)\n",
    "f1 = f1_score(labels, predicts)\n",
    "\n",
    "# First print all metrics with no approximation\n",
    "print(f'Metrics: {accuracy}, {precision}, {recall}, {f1}\\n')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate accounting for JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115,\n",
       " 0,\n",
       " 0,\n",
       " 0.9937219730941704,\n",
       " 0.9863013698630136,\n",
       " 0.9664429530201343,\n",
       " 0.9762711864406781)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-load the generated predictions file\n",
    "predictions_file_path = 'generated_predictions.jsonl'\n",
    "with open(predictions_file_path, 'r') as file:\n",
    "    predictions = [line for line in file]\n",
    "\n",
    "# Initialize counters\n",
    "valid_responses = 0\n",
    "invalid_json_structure = 0\n",
    "invalid_json_content = 0\n",
    "\n",
    "# Initialize lists for labels and predictions for valid cases\n",
    "valid_labels = []\n",
    "valid_predicts = []\n",
    "\n",
    "# Parsing the predictions file\n",
    "for line in predictions:\n",
    "    try:\n",
    "        # Attempt to parse the JSON\n",
    "        prediction = json.loads(line)\n",
    "\n",
    "        # Check if both label and predict are valid JSONs and contain the correct key\n",
    "        if 'is_fraudulent' in json.loads(prediction['label']) and 'is_fraudulent' in json.loads(prediction['predict']):\n",
    "            valid_responses += 1\n",
    "            valid_labels.append(json.loads(prediction['label'])['is_fraudulent'])\n",
    "            valid_predicts.append(json.loads(prediction['predict'])['is_fraudulent'])\n",
    "        else:\n",
    "            invalid_json_content += 1\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        # Count invalid JSON structures\n",
    "        invalid_json_structure += 1\n",
    "\n",
    "# Convert boolean to integer for metrics calculation\n",
    "valid_labels = [int(label) for label in valid_labels]\n",
    "valid_predicts = [int(predict) for predict in valid_predicts]\n",
    "\n",
    "# Calculating metrics for valid responses\n",
    "accuracy_valid = accuracy_score(valid_labels, valid_predicts) if valid_responses > 0 else None\n",
    "precision_valid = precision_score(valid_labels, valid_predicts) if valid_responses > 0 else None\n",
    "recall_valid = recall_score(valid_labels, valid_predicts) if valid_responses > 0 else None\n",
    "f1_valid = f1_score(valid_labels, valid_predicts) if valid_responses > 0 else None\n",
    "\n",
    "valid_responses, invalid_json_structure, invalid_json_content, accuracy_valid, precision_valid, recall_valid, f1_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid responses: 1115\n",
      "Invalid JSON structures: 0\n",
      "Invalid JSON content: 0\n",
      "Accuracy for valid responses: 0.99\n",
      "Precision for valid responses: 0.99\n",
      "Recall for valid responses: 0.97\n",
      "F1 Score for valid responses: 0.98\n"
     ]
    }
   ],
   "source": [
    "print(f'Valid responses: {valid_responses}')\n",
    "print(f'Invalid JSON structures: {invalid_json_structure}')\n",
    "print(f'Invalid JSON content: {invalid_json_content}')\n",
    "print(f'Accuracy for valid responses: {accuracy_valid:.2f}' if accuracy_valid is not None else 'No valid responses to calculate accuracy')\n",
    "print(f'Precision for valid responses: {precision_valid:.2f}' if precision_valid is not None else 'No valid responses to calculate precision')\n",
    "print(f'Recall for valid responses: {recall_valid:.2f}' if recall_valid is not None else 'No valid responses to calculate recall')\n",
    "print(f'F1 Score for valid responses: {f1_valid:.2f}' if f1_valid is not None else 'No valid responses to calculate F1 score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
