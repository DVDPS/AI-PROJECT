{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC LIBRARIES FOR EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENING FILES AND SAVING THE FIRST ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv(\"sms.csv\")\n",
    "head = sms.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_col = sms.isnull().sum()\n",
    "print(\"Null values in each column: \\n\", null_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributing and Correlating Data\n",
    "**This script is useful for analyzing trends in SMS data, particularly to understand the distribution and frequency of fraudulent messages over different time periods. But there's no correlation that result from the analyzed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['Date and Time'] = pd.to_datetime(sms['Date and Time'])\n",
    "\n",
    "# Extracting different time components\n",
    "sms['year'] = sms['Date and Time'].dt.year\n",
    "sms['month'] = sms['Date and Time'].dt.month\n",
    "sms['day'] = sms['Date and Time'].dt.day\n",
    "\n",
    "\n",
    "# Filtering for fraudulent messages\n",
    "fraudulent_sms = sms[sms['Fraudolent'] == 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.delaxes(axes[1][1])\n",
    "# Plot 1: Fraud percentage by year\n",
    "sns.countplot(x='year', hue='Fraudolent', data=sms, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Fraudolent messages by year')\n",
    "\n",
    "# Plot 2: Fraud percentage by month\n",
    "sns.countplot(x='month', hue='Fraudolent', data=sms, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Fraudolent messages by month')\n",
    "\n",
    "# Plot 4: Fraud percentage by day\n",
    "sns.countplot(x='day', hue='Fraudolent', data=sms, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Fraudolent messages by day')\n",
    "\n",
    "# Adjust spacing between subplots for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE MODEL APPROACH (Logistic Regression)\n",
    "**We implemented a basic model to compare it's performance against the later used language model**\n",
    "### Data Loading and Preparation\n",
    "\n",
    "**-The script starts by importing necessary libraries from pandas and sklearn.**\n",
    "\n",
    "**-It reads the SMS data from a CSV file into a pandas DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"sms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "**-A TfidfVectorizer is initialized, excluding English stop words.**\n",
    "\n",
    "**-The 'SMS test' column (likely a typo, should be 'SMS text') is transformed into TF-IDF features, a numerical representation suitable for machine learning models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['SMS test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable\n",
    "\n",
    "**The 'Fraudolent' column is set as the target variable y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Fraudolent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "**The dataset is split into training and testing sets, with 20% of the data reserved for testing.**\n",
    "\n",
    "**The random state is set for reproducibility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "**A Logistic Regression model is instantiated and trained using the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction and Evaluation\n",
    "\n",
    "**The model makes predictions on the test set.**\n",
    "**A classification report is printed, showing key metrics like precision, recall, and F1-score for evaluating the model's performance on detecting fraudulent SMS messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Resampling\n",
    "\n",
    "**-The SMOTE method from imblearn.over_sampling is used to handle class imbalance.**\n",
    "\n",
    "**-smote = SMOTE(random_state=42) creates a SMOTE object with a fixed random state for reproducibility.**\n",
    "\n",
    "**-X_resampled, y_resampled = smote.fit_resample(X, df['Fraudolent']) applies SMOTE to the feature matrix X and target vector df['Fraudolent']. This results in a balanced dataset by creating synthetic samples for the minority class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, df['Fraudolent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "**train_test_split from sklearn.model_selection is used to split the data into training and testing sets.**\n",
    "\n",
    "**-X_resampled and y_resampled are split into training (X_train, y_train) and testing (X_test, y_test) sets, with 20% of the data held out for testing.**\n",
    "\n",
    "**-stratify=y_resampled ensures that the proportion of classes in both the training and testing sets reflects the proportion in the resampled dataset, which is important for maintaining balance in both sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "**-A LogisticRegression model from sklearn.linear_model is instantiated.**\n",
    "\n",
    "**-model.fit(X_train, y_train) trains the logistic regression model on the training data. This step involves the model learning to differentiate between classes based on the features provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction and Evaluation\n",
    "\n",
    "**predictions = model.predict(X_test) uses the trained model to predict the class labels for the test set.**\n",
    "\n",
    "**-print(classification_report(y_test, predictions)) prints a classification report that includes key metrics like precision, recall, and F1-score for each class. These metrics are crucial for evaluating the model's performance, especially in a balanced dataset scenario where accuracy alone is not a sufficient measure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM (Language Model) Approach\n",
    "\n",
    "- We started by loading and testing the base model, using LLaMA-Factory and LM-Studio, to assure easy reproducibility.\n",
    "  - Evaluated the model on the SMS data (test set).\n",
    "  - Extracted Metrics (Accuracy, Precision, Recall, F1-Score) and Confusion Matrix, in addition to \"JSON output\" evaluation.\n",
    "\n",
    "\n",
    "- We fine-tuned the model using the training set, prompt \"x\" and same parameters as the base model.\n",
    "  - Evaluated the model on the SMS data (test set).\n",
    "  - Extracted Metrics (Accuracy, Precision, Recall, F1-Score) and Confusion Matrix, in addition to \"JSON output\" evaluation.\n",
    "  - Assessed improvements etc.\n",
    "\n",
    "\n",
    "- Converted the model to GGUF for easier CPU inference and to make the model more \"computationally friendly\", so it can also be run on 3rd party services like \"LM Studio\".\n",
    "\n",
    "\n",
    "- Created 2 version of the model:\n",
    "  - F16 (16-bit precision) which is the default fine-tuned version.\n",
    "  - Q8 (8-bit precision) which is a quantized version of the fine-tuned model, to make it even more \"computationally friendly\" but a little less reliable.\n",
    "  - All uploaded to \"Hugging Face\" for easy access and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters & Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen prompt:\n",
    "\n",
    "BOOL_SYSTEM_MESSAGE = \"\"\"You are excellent message moderator, expert in detecting fraudulent messages.\n",
    "\n",
    "You will be given \"Messages\" and your job is to predict if a message is fraudulent or not.\n",
    "\n",
    "You ONLY respond FOLLOWING this json schema:\n",
    "\n",
    "{\n",
    "    \"is_fraudulent\": {\n",
    "        \"type\": \"boolean\",\n",
    "        \"description\": \"Whether the message is predicted to be fraudulent.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "You MUST ONLY RESPOND with a boolean value in JSON. Either true or false in JSON. NO EXPLANATIONS OR COMMENTS.\n",
    "\n",
    "Example of valid responses:\n",
    "{\n",
    "    \"is_fraudulent\": true\n",
    "}\n",
    "or \n",
    "{\n",
    "    \"is_fraudulent\": false\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# After small test this was the only one that didn't yield immediate terrible results for both JSON and task performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used consistently during test, fine-tuning and final run.\n",
    "# Could have played more with it, but considering time, we decided to stick with these.\n",
    "\n",
    "temperature = 0.3\n",
    "max_tokens = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM Studio - easy inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- LM Studio\n",
    "- LM Studio's server running on \"http://localhost:1234/v1\" or any other easy accessible address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to obtain the result in JSON format, which is a common data format with diverse uses in data handling. The expected output should look like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"is_fraudulent\": true\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"is_fraudulent\": false\n",
    "}\n",
    "```\n",
    "\n",
    "The language model categorizes the sms as fraudulent or not.\n",
    "\n",
    "This is done by a combination of:\n",
    "- Prompt engineering\n",
    "- Fine-tuning\n",
    "- Hyperparameter optimization\n",
    "- Function-calling or JSON outputs\n",
    "- Run inferenece on API - LM Studio (mimics OpenAI API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Implementation:\n",
    "\n",
    "**Prompts:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INSTRUCTIONS = \"\"\"You are excellent message moderator, expert in detecting fraudulent messages.\n",
    "\n",
    "You will be given \"Messages\" and your job is to predict if a message is fraudulent or not.\n",
    "\n",
    "You ONLY respond FOLLOWING this json schema:\n",
    "\n",
    "{\n",
    "    \"is_fraudulent\": {\n",
    "        \"type\": \"boolean\",\n",
    "        \"description\": \"Whether the message is predicted to be fraudulent.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "You MUST ONLY RESPOND with a boolean value in JSON. Either true or false in JSON. NO EXPLANATIONS OR COMMENTS.\n",
    "\n",
    "Example of valid responses:\n",
    "{\n",
    "    \"is_fraudulent\": true\n",
    "}\n",
    "or \n",
    "{\n",
    "    \"is_fraudulent\": false\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full process as a function\n",
    "\n",
    "**-Run inference, get JSON output, and save the result result (with error handling).**\n",
    "\n",
    "**-user_query is passed to predict_fraudolence function, which uses the LLM to predict whether the message is fraudolent or not**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import Optional\n",
    "\n",
    "local_server = \"http://localhost:1234/v1\"\n",
    "client = OpenAI(base_url=local_server, api_key=\"sk_1234567890\")\n",
    "\n",
    "# Choose which system message to use based on whether you want a confidence score\n",
    "system_message = INSTRUCTIONS\n",
    "def predict_fraudulence(user_query):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "            ]\n",
    "        )\n",
    "        prediction_content = response.choices[0].message.content\n",
    "        prediction_json = json.loads(prediction_content)\n",
    "        \n",
    "        # Validate the prediction JSON\n",
    "        if \"is_fraudulent\" in prediction_json and \\\n",
    "            (type(prediction_json[\"is_fraudulent\"]) == bool):\n",
    "            return prediction_json\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid JSON structure from prediction: {prediction_content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**user_query is a variable set to a specific text message**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction\n",
    "user_query = \"Hi Chris! Can you send me $1,000,000? I need it for a school project. Thanks!\"\n",
    "prediction_json = predict_fraudulence(user_query)\n",
    "\n",
    "if prediction_json:\n",
    "    print(prediction_json)\n",
    "else:\n",
    "    print(\"Failed to get a valid prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction_json calls the function and the user query and returns the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_json[\"is_fraudulent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "\n",
    "# Load the generated predictions file\n",
    "predictions_file_path = 'final_model_eval_2023-12-02-21-34-10\\generated_predictions.jsonl'\n",
    "with open(predictions_file_path, 'r') as file:\n",
    "    predictions = [json.loads(line) for line in file]\n",
    "\n",
    "# Display the first few entries to understand the structure\n",
    "predictions[:5]\n",
    "# Extracting labels and predictions\n",
    "labels = [json.loads(entry['label'])['is_fraudulent'] for entry in predictions]\n",
    "predicts = [json.loads(entry['predict'])['is_fraudulent'] for entry in predictions]\n",
    "\n",
    "# Converting boolean to integer for metrics calculation\n",
    "labels = [int(label) for label in labels]\n",
    "predicts = [int(predict) for predict in predicts]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(labels, predicts)\n",
    "precision = precision_score(labels, predicts)\n",
    "recall = recall_score(labels, predicts)\n",
    "f1 = f1_score(labels, predicts)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Value\" : [accuracy, precision, recall, f1]\n",
    "})\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM PYTORCH TO GGUF\n",
    "**Python is a poor choice for AI inference stacks.**\n",
    "\n",
    "**In production systems, we would want to remove the reliance on PyTorch and Python.**\n",
    "\n",
    "**With the aid of programs like llama.cpp, GGUF may facilitate extremely effective zero-Python inference.**\n",
    "\n",
    "**GGUF facilitates the usage of CPU inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After having installed the repository with the following command in the terminal:**\n",
    "\n",
    "`git clone https://github.com/ggerganov/llama.cpp`\n",
    "\n",
    "`cd llama.cpp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we then use the command `convert.py` to convert the PYTORCH model to GGUF, simply by giving the repository containing the PYTORCH files. The GGUF model file is a full 16-bit floating point model. It is not yet quantized**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python convert.py _er modello_`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then with the help of mkDev64build application we built the quantizer by giving it the fine-tuned model and selecting the type of quatization we wanted (8-bit)**\n",
    "**All these models are accessible in our HUGGINGFACE repository (https://huggingface.co/SimplyLeo/Zephyr-Fraudulence-Detector)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
